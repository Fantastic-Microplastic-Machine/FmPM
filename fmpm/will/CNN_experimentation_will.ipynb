{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "presidential-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fmpm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn\n",
    "import skimage.io\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-departure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Will/GradSchool/CHEME/Project/FmPM/fmpm/will/fmpm.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  identification[i] = False\n",
      "/Users/Will/GradSchool/CHEME/Project/FmPM/fmpm/will/fmpm.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  identification[i] = True\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "image_dir = 'data/10x'\n",
    "labels_file = 'data/me.csv'\n",
    "\n",
    "DATA = fmpm.prep_data(pd.read_csv(labels_file), image_dir)\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToPILImage(),\n",
    "                            torchvision.transforms.RandomRotation((-180,180)),\n",
    "                            torchvision.transforms.CenterCrop((325)),\n",
    "                            torchvision.transforms.ToTensor()\n",
    "                                      ])\n",
    "\n",
    "train_data = fmpm.tenX_dataset(DATA, 'data/10x', transform =transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aquatic-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(im):\n",
    "    #Could as try nn.function.normalize\n",
    "    count = 0\n",
    "    for channel in im:\n",
    "        mean = torch.mean(channel).item()\n",
    "        std = torch.std(channel).item()\n",
    "        im[count] = (channel - mean) / std\n",
    "        count += 1\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imperial-yellow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1180, -1.1180,  1.1180],\n",
      "         [ 0.0000, -1.1180,  0.0000]],\n",
      "\n",
      "        [[ 1.2857, -0.7443,  1.2857],\n",
      "         [-0.5413, -0.7443, -0.5413]]])\n"
     ]
    }
   ],
   "source": [
    "def test_normalize_image():\n",
    "    test_im = torch.Tensor([[[10, 0, 10],\n",
    "                             [5, 0, 5]],\n",
    "                           [[100, 0, 100],\n",
    "                            [10, 0, 10]]\n",
    "                           ])\n",
    "    print(normalize_image(test_im))\n",
    "    \n",
    "    \n",
    "test_normalize_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "jewish-concentration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9529, 0.9451, 0.9725,  ..., 0.9804, 0.9608, 0.9686],\n",
      "         [0.9725, 0.9686, 0.9608,  ..., 0.9882, 0.9686, 0.9686],\n",
      "         [0.9725, 0.9686, 0.9686,  ..., 0.9882, 0.9804, 0.9725],\n",
      "         ...,\n",
      "         [0.9765, 0.9804, 0.9843,  ..., 0.9961, 0.9882, 0.9922],\n",
      "         [0.9961, 0.9804, 0.9922,  ..., 0.9922, 0.9882, 0.9922],\n",
      "         [0.9961, 0.9961, 0.9961,  ..., 0.9922, 0.9922, 0.9804]],\n",
      "\n",
      "        [[0.4196, 0.4118, 0.4000,  ..., 0.4275, 0.4275, 0.4353],\n",
      "         [0.4392, 0.4196, 0.4000,  ..., 0.4275, 0.4196, 0.4196],\n",
      "         [0.4392, 0.4275, 0.4196,  ..., 0.4275, 0.4196, 0.4118],\n",
      "         ...,\n",
      "         [0.4392, 0.4196, 0.4196,  ..., 0.4588, 0.4510, 0.4667],\n",
      "         [0.4588, 0.4392, 0.4392,  ..., 0.4510, 0.4667, 0.4667],\n",
      "         [0.4588, 0.4588, 0.4431,  ..., 0.4510, 0.4667, 0.4667]],\n",
      "\n",
      "        [[0.2471, 0.2078, 0.1961,  ..., 0.2706, 0.2353, 0.2353],\n",
      "         [0.2235, 0.2235, 0.2078,  ..., 0.2353, 0.2627, 0.2627],\n",
      "         [0.2471, 0.2471, 0.2353,  ..., 0.2471, 0.2471, 0.2471],\n",
      "         ...,\n",
      "         [0.2627, 0.2588, 0.2471,  ..., 0.2784, 0.2667, 0.2784],\n",
      "         [0.2980, 0.2706, 0.2706,  ..., 0.2706, 0.2706, 0.2784],\n",
      "         [0.2902, 0.2902, 0.2784,  ..., 0.2706, 0.2588, 0.2627]]])\n"
     ]
    }
   ],
   "source": [
    "for samp in range(len(train_data)):\n",
    "    #plt.figure(samp)\n",
    "    im = train_data[samp]['image']\n",
    "    #plt.imshow(pimp_my_image(im))\n",
    "    print(im)\n",
    "    if samp is 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-determination",
   "metadata": {},
   "source": [
    "#### The CNN archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vulnerable-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes CNN. Here we just define layer shapes that we call in the forward func\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 3, \n",
    "                               out_channels = 6, \n",
    "                               kernel_size = 5)\n",
    "                \n",
    "        #Convultion layer 2. See above\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels = 6, \n",
    "                               out_channels = 12, \n",
    "                               kernel_size = 5)\n",
    "        \n",
    "        self.fc_1 = torch.nn.Linear(39 * 39 * 12, 256)\n",
    "        self.fc_2 = torch.nn.Linear(256, 2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Function that performs all the neural network forward calculation i.e.\n",
    "        takes image data from the input of the neural network to the output\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size = 2)\n",
    "        x = torch.nn.functional.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size = 4)\n",
    "        x = torch.nn.functional.leaky_relu(x)\n",
    "        x = x.view(x.shape[0], -1)  \n",
    "        x = self.fc_1(x) \n",
    "        x = torch.nn.functional.leaky_relu(x)\n",
    "        x = self.fc_2(x)    \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greater-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring iterator. The thing that will loop through our dataset.\n",
    "\n",
    "train_data = fmpm.tenX_dataset(DATA, 'data/10x', transform =transforms)\n",
    "BATCH_SIZE = 10\n",
    "train_iterator = torch.utils.data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "\n",
    "#Instancing model, loss criteria, device to perform calculations on, and optimizer.\n",
    "model = LeNet()\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.005)\n",
    "\n",
    "#Telling the model and loss function to do math on whatever device is\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smoking-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    #argmax returns the indices of the maximum value of all elements in the input tensor.\n",
    "    #We do this along the column of 2 element tensors returning a single column of predictions\n",
    "    #Then we compare to our target, convert to a number, calculate the average for the batch\n",
    "    acc = ((y_pred.argmax(dim=1) == y).float().mean())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "living-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop. Takes data through NN calculates loss and adjusts NN. Repeat\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    #Need to add logic to skip iteration if image is None\n",
    "    for sample in iterator:  \n",
    "        image = sample['image'].to(device)\n",
    "        isPlasticRaw = sample['plastic'].to(device)\n",
    "        image = normalize_image(image)\n",
    "        optimizer.zero_grad()      \n",
    "        y_pred = model(image)\n",
    "        isPlastic = isPlasticRaw.argmax(dim=1)\n",
    "        loss = criterion(y_pred, isPlastic)\n",
    "        acc = calculate_accuracy(y_pred, isPlastic)\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator) , epoch_acc / len(iterator), y_pred, isPlasticRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, acc: 0.7415770583095089, loss: 1.449638243223871\n",
      "tensor([[ 0.5219, -0.2852],\n",
      "        [ 0.7293, -0.3940],\n",
      "        [ 0.5198, -0.2876],\n",
      "        [ 0.4769, -0.2511],\n",
      "        [ 0.5242, -0.2876],\n",
      "        [ 0.5560, -0.3177],\n",
      "        [ 0.4954, -0.2652],\n",
      "        [ 0.5019, -0.2706],\n",
      "        [ 9.2358, -6.7210]], grad_fn=<AddmmBackward>)\n",
      "tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0]])\n",
      "EPOCH: 1, acc: 0.8025089598471119, loss: 0.5159527063369751\n",
      "EPOCH: 2, acc: 0.802508956001651, loss: 0.5042894965698642\n",
      "EPOCH: 3, acc: 0.8096774182012004, loss: 0.5112192068369158\n",
      "EPOCH: 4, acc: 0.8060931851786952, loss: 0.47697674002378215\n",
      "EPOCH: 5, acc: 0.8537634322720189, loss: 0.47753952203258393\n",
      "tensor([[ 1.6012, -3.0040],\n",
      "        [ 7.3741, -6.0684],\n",
      "        [ 1.1949, -2.5606],\n",
      "        [ 4.5312, -4.6729],\n",
      "        [ 1.1902, -2.5735],\n",
      "        [ 1.2004, -2.5662],\n",
      "        [ 1.1984, -2.5630],\n",
      "        [ 1.4624, -2.8148],\n",
      "        [ 0.8966, -2.1167]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1]])\n",
      "EPOCH: 6, acc: 0.8383512516175547, loss: 0.4404037820716058\n",
      "EPOCH: 7, acc: 0.8544802838756193, loss: 0.407668860088433\n",
      "EPOCH: 8, acc: 0.831899639098875, loss: 0.409944461478341\n",
      "EPOCH: 9, acc: 0.8609318906261075, loss: 0.45206796822528683\n",
      "EPOCH: 10, acc: 0.854121858073819, loss: 0.39118150645686733\n",
      "tensor([[ 0.2810, -0.9209],\n",
      "        [ 0.3719, -1.1592],\n",
      "        [ 0.4470, -1.4134],\n",
      "        [ 0.1146, -0.6691],\n",
      "        [ 0.4537, -1.4165],\n",
      "        [ 1.1642, -2.2481],\n",
      "        [ 0.4591, -1.4439],\n",
      "        [ 0.0937, -0.2372],\n",
      "        [ 0.2908, -0.8865]], grad_fn=<AddmmBackward>)\n",
      "tensor([[1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1]])\n",
      "EPOCH: 11, acc: 0.8516128928430619, loss: 0.3966853849349483\n",
      "EPOCH: 12, acc: 0.86021505055889, loss: 0.36953997527880056\n",
      "EPOCH: 13, acc: 0.8541218561510886, loss: 0.41657244317954584\n",
      "EPOCH: 14, acc: 0.8802867320276075, loss: 0.3579239775576899\n",
      "EPOCH: 15, acc: 0.8645161217258822, loss: 0.5446304376086881\n",
      "tensor([[ 0.7177, -1.5492],\n",
      "        [ 0.6928, -1.4989],\n",
      "        [ 0.5980, -0.8526],\n",
      "        [ 0.6776, -1.4320],\n",
      "        [ 0.6137, -1.2960],\n",
      "        [ 0.6433, -1.4172],\n",
      "        [ 0.7198, -1.5737],\n",
      "        [ 0.0194,  0.2585],\n",
      "        [ 0.6011, -1.5485]], grad_fn=<AddmmBackward>)\n",
      "tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0]])\n",
      "EPOCH: 16, acc: 0.8473118255215306, loss: 1.0392853713804675\n",
      "EPOCH: 17, acc: 0.7835125408826336, loss: 1.5800788208237668\n",
      "EPOCH: 18, acc: 0.7931899620640662, loss: 0.7911493769154937\n",
      "EPOCH: 19, acc: 0.8351254405513886, loss: 0.3549004909732649\n",
      "EPOCH: 20, acc: 0.8354838692372845, loss: 0.6206524862359548\n",
      "tensor([[-2.0710, -6.5792],\n",
      "        [-2.3890, -4.1697],\n",
      "        [-1.9136, -5.4767],\n",
      "        [-2.1330, -5.1759],\n",
      "        [-2.4360, -4.8096],\n",
      "        [-1.9175, -4.7740],\n",
      "        [-2.1918, -5.0477],\n",
      "        [-2.0982, -5.3881],\n",
      "        [-2.2685, -5.1918]], grad_fn=<AddmmBackward>)\n",
      "tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0]])\n",
      "EPOCH: 21, acc: 0.8476702494006003, loss: 0.7043524904236678\n",
      "EPOCH: 22, acc: 0.8677419270238569, loss: 0.3492800429944069\n",
      "EPOCH: 23, acc: 0.8505376346649662, loss: 0.6204297917204038\n",
      "EPOCH: 24, acc: 0.8028673875716424, loss: 0.5071944115202753\n",
      "EPOCH: 25, acc: 0.7935483835397228, loss: 0.5070938592506272\n",
      "tensor([[-0.0675, -2.2991],\n",
      "        [ 0.0904, -3.2655],\n",
      "        [ 0.0982, -2.2716],\n",
      "        [-1.9595, -0.5636],\n",
      "        [ 0.0394, -2.9650],\n",
      "        [ 0.2375, -3.6864],\n",
      "        [-0.2422, -1.8663],\n",
      "        [ 0.2128, -3.6137],\n",
      "        [ 0.1682, -2.9089]], grad_fn=<AddmmBackward>)\n",
      "tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0]])\n",
      "EPOCH: 26, acc: 0.8415770588382598, loss: 0.4225931845965885\n"
     ]
    }
   ],
   "source": [
    "#Here the model is actually trained\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS+1):\n",
    "    train_loss, train_acc, y_pred, target = train(model, train_iterator, optimizer, criterion, device)\n",
    "    print(f'EPOCH: {epoch}, acc: {train_acc}, loss: {train_loss}')\n",
    "    if epoch % 5 is 0:\n",
    "        print(y_pred)\n",
    "        print(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-multimedia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_class():\n",
    "    filename = '../tests/test_assets/test_labels_unclean.csv'\n",
    "    image_dir = '../tests/test_assets/test_images'\n",
    "    \n",
    "    #Eventually just read in already cleaned labels sheet\n",
    "    labels = prep_data(pd.read_csv(filename), image_dir)\n",
    "    \n",
    "    \n",
    "    transform = None\n",
    "    test_dataset = tenX_dataset(labels, image_dir, transform)\n",
    "    \n",
    "    #len() check\n",
    "    length = len(test_dataset)\n",
    "    expect = 5\n",
    "    assert length is expect, f'10x dataset length method failed. Got {length}, should be {expect}'\n",
    "    \n",
    "    #get_item() check\n",
    "    samples = []\n",
    "    keys = ['image', 'plastic','shape','color']\n",
    "    for i in range(len(test_dataset)):\n",
    "        assert test_dataset[i]['image'] is not None, 'Got NoneType instead of image'\n",
    "        isP = test_dataset[i]['plastic']\n",
    "        assert math.isclose(0,isP) or math.isclose(1,isP), f'plastic not 0 or 1, instead is {isP}'\n",
    "        length = len(test_dataset[i]['shape'])\n",
    "        assert length is 4, f'length of shape array not 4, instead is {length}'\n",
    "        samples.append(test_dataset[i])\n",
    "        \n",
    "    shape = samples[0]['shape']\n",
    "    assert math.isclose(shape[2], 1),  f'wrong shape first image, is {shape}'\n",
    "    color = samples[0]['color']\n",
    "    assert math.isclose(color[0], 1), f'wrong color first image, is {color}'\n",
    "    assert math.isclose(samples[4]['plastic'], 1), 'wrong plastic id 3rd image'\n",
    "    \n",
    "test_dataset_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.to_csv('try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_frame['isPlastic'].astype(bool).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "output = torch.randn(10, 2).float()\n",
    "target = torch.FloatTensor(10).uniform_(0, 2).long()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(output)\n",
    "print(output.size())\n",
    "print(target)\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-adoption",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
