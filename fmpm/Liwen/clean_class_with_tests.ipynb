{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ahead-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional \n",
    "import torch.optim \n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision.transforms\n",
    "import torchvision.datasets\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-bundle",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hindu-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"sets seeds for several used packages for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deadly-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_seeds():\n",
    "    \"\"\"\n",
    "    Test if the set_seeds function works.\n",
    "    \"\"\"\n",
    "    seed = 42\n",
    "    set_seeds(seed) # Call the set_seeds function.\n",
    "    # create random datasets using torch.randint, random.randint, and np.random.randint. \n",
    "    x = torch.randint(0, 10, (3, 3))\n",
    "    y = random.randint(0,100) \n",
    "    z = np.random.randint(5, size=(2, 4))\n",
    "    set_seeds(seed) # Set the same seeds again.\n",
    "    # Check the random datasets are still the same.\n",
    "    assert torch.equal(x, torch.randint(0, 10, (3, 3))), \"The set_seed function is broken!\"\n",
    "    assert y == random.randint(0,100), \"The set_seed function is broken!\"\n",
    "    assert np.array_equal(z, np.random.randint(5, size=(2, 4))), \"The set_seed function is broken!\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limited-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "independent-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_column(column):\n",
    "    \"\"\"\n",
    "    takes single columned Pandas DataFrame of categorical data and encodes it\n",
    "    into array of class binarys\n",
    "    \"\"\"\n",
    "    encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "    shape_arr = encoder.fit_transform(column).toarray().astype(int)\n",
    "        \n",
    "    return list(shape_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "informal-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_column_1():\n",
    "    \"\"\"\n",
    "    Test if the encode_column function can generate correct output.\n",
    "    \"\"\"\n",
    "    labels = pd.read_csv('data/10x_labels_4.csv')  # Import the csv file containing labels.\n",
    "    # Create two new columns, color and shape.\n",
    "    new = labels[\"Description\"].str.split(\" \", n=1, expand=True)\n",
    "    input_column_color = new[0].values\n",
    "    input_column_shape = new[1].values\n",
    "    # Call the encode_column function to turn the color and shape features into binary codes.\n",
    "    output_color = encode_column(input_column_color.reshape(-1, 1))\n",
    "    output_shape = encode_column(input_column_shape.reshape(-1, 1))\n",
    "    # Expected output of the encode_column function.\n",
    "    expect_output_color = [[0, 0, 0, 1, 0, 0],\n",
    "                           [0, 0, 0, 1, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 1],\n",
    "                           [0, 1, 0, 0, 0, 0],\n",
    "                           [0, 1, 0, 0, 0, 0],\n",
    "                           [0, 0, 1, 0, 0, 0],\n",
    "                           [0, 0, 1, 0, 0, 0],\n",
    "                           [1, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 1, 0, 0],\n",
    "                           [0, 0, 0, 1, 0, 0],\n",
    "                           [0, 0, 0, 1, 0, 0],\n",
    "                           [0, 0, 0, 1, 0, 0],\n",
    "                           [0, 0, 1, 0, 0, 0],\n",
    "                           [0, 0, 1, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 1, 0],\n",
    "                           [0, 0, 0, 0, 1, 0]]\n",
    "    expect_output_shape = [[1, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 1],\n",
    "                           [0, 0, 0, 0, 1],\n",
    "                           [0, 1, 0, 0, 0],\n",
    "                           [0, 1, 0, 0, 0],\n",
    "                           [0, 1, 0, 0, 0],\n",
    "                           [1, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 1, 0],\n",
    "                           [0, 0, 0, 1, 0],\n",
    "                           [0, 0, 0, 0, 1],\n",
    "                           [0, 0, 1, 0, 0],\n",
    "                           [0, 0, 1, 0, 0],\n",
    "                           [0, 0, 0, 0, 1],\n",
    "                           [1, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 1, 0],\n",
    "                           [0, 0, 1, 0, 0]]\n",
    "    # Check if the expected output is the same as the actual output.\n",
    "    assert np.array_equal(expect_output_color, output_color), \"The function encode_column is broken!\"\n",
    "    assert np.array_equal(expect_output_shape, output_shape), \"The function encode_column is broken!\"\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "actual-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encode_column_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automotive-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_column_2():\n",
    "    \"\"\"\n",
    "    Test if the encode_column function is responsive to a wrong datatype of the input.\n",
    "    \"\"\"\n",
    "    input_1 = 10\n",
    "    test1 = False\n",
    "    try:\n",
    "        encode_column(input_1)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, ValueError), \"Wrong type of error.\"\n",
    "        test1 = True\n",
    "    assert test1 == True, \"Test failed! The encode_column function is not responsive to the wrong input datatype 'int'.\"\n",
    "    \n",
    "    labels = pd.read_csv('data/10x_labels_4.csv')\n",
    "    new = labels[\"Description\"].str.split(\" \", n=1, expand=True)\n",
    "    input_2 = new[0].values\n",
    "    test2 = False\n",
    "    try:\n",
    "        encode_column(input_2)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, ValueError), \"Wrong type of error.\"\n",
    "        test2 = True\n",
    "    assert test2 == True, \"Test failed! The encode_column function is not responsive to the wrong input datatype '1D array'.\"\n",
    "    \n",
    "    input_3 = [1, 2, 3]\n",
    "    test3 = False\n",
    "    try:\n",
    "        encode_column(input_3)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, ValueError), \"Wrong type of error.\"\n",
    "        test3 = True\n",
    "    assert test3 == True, \"Test failed! The encode_column function is not responsive to the wrong input datatype 'list'.\"\n",
    "    \n",
    "    input_4 = 'input'\n",
    "    test4 = False\n",
    "    try:\n",
    "        encode_column(input_4)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, ValueError), \"Wrong type of error.\"\n",
    "        test4 = True\n",
    "    assert test4 == True, \"Test failed! The encode_column function is not responsive to the wrong input datatype 'str'.\"\n",
    "    \n",
    "    input_5 = True\n",
    "    test5 = False\n",
    "    try:\n",
    "        encode_column(input_5)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, ValueError), \"Wrong type of error.\"\n",
    "        test5 = True\n",
    "    assert test5 == True, \"Test failed! The encode_column function is not responsive to the wrong input datatype 'bool'.\"\n",
    "    \n",
    "    input_6 = 1.4\n",
    "    test6 = False\n",
    "    try:\n",
    "        encode_column(input_6)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, ValueError), \"Wrong type of error.\"\n",
    "        test6 = True\n",
    "    assert test6 == True, \"Test failed! The encode_column function is not responsive to the wrong input datatype 'float'.\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "funded-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encode_column_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "shared-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(labels, image_root):\n",
    "    \"\"\"\n",
    "    Takes in raw labels dataframe and converts it into the format\n",
    "    expected for tenX_dataset class\n",
    "    \"\"\"\n",
    "\n",
    "    #Splitting description column into color and shape columns\n",
    "    new = labels[\"Description\"].str.split(\" \", n=1, expand=True)\n",
    "    labels.drop(columns=['Description'], inplace=True)\n",
    "    labels['Color'] = new[0].values\n",
    "    labels['Shape'] = new[1].values\n",
    "    \n",
    "    #Decomposing sample keywords into seperate strings\n",
    "    sample_names = labels[\"Sample\"].str.split(\" \", n=1, expand=False)\n",
    "    labels['Sample'] = sample_names\n",
    "    \n",
    "    #Converting identification into boolean for is/is not plastic\n",
    "    PLASTICS = ['polystyrene', 'polyethylene','polypropylene','Nylon','ink + plastic','PET','carbon fiber']\n",
    "    identification = labels['Identification']\n",
    "    \n",
    "    for i in range(0,len(identification)):\n",
    "        if identification[i] in PLASTICS:\n",
    "            identification[i] = True\n",
    "        else:\n",
    "            identification[i] = False\n",
    "\n",
    "    labels['Identification'] = identification\n",
    "    labels.rename(columns={'Identification': 'isPlastic'}, inplace=True)\n",
    "    labels['isPlastic'] = labels[\"isPlastic\"].astype(int)\n",
    "    \n",
    "    \n",
    "    #Encoding shape and color data\n",
    "    labels['Shape'] = encode_column(labels[['Shape']])\n",
    "    labels['Color'] = encode_column(labels[['Color']])\n",
    "    \n",
    "    labels = add_filenames(labels, image_root)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "veterinary-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prep_data_1():\n",
    "    \"\"\"\n",
    "    Test if the prep_data function can generate correct output.\n",
    "    \"\"\"\n",
    "    # Load the csv file as the input data frame.\n",
    "    input_df = pd.read_csv('data/10x_labels_5.csv')\n",
    "    image_dir = 'data/images_10x'\n",
    "    # Call the prep_data function and get the actual output \"result\".\n",
    "    result = prep_data(input_df, image_dir)\n",
    "    # Load the output data frame from a csv file.\n",
    "    output_df = pd.read_csv('data/prep_data_output.csv')\n",
    "    # Modify the format of the output data frame to make it the expected output.\n",
    "    for i, rowi in output_df['Sample'].iteritems():\n",
    "        output_df['Sample'].loc[i] = rowi.split(',')\n",
    "    for j, rowj in output_df['Color'].iteritems():\n",
    "        output_df['Color'].loc[j] = np.fromstring(rowj, dtype=int, sep=' ')\n",
    "    for k, rowk in output_df['Shape'].iteritems():\n",
    "        output_df['Shape'].loc[k] = np.fromstring(rowk, dtype=int, sep=' ')\n",
    "    # Check if the expected output data frame is the same as the actual output data frame. \n",
    "    assert output_df.equals(result), \"The prep_data function is broken!\"\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vulnerable-third",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liwenxing/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/liwenxing/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/liwenxing/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/Users/liwenxing/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  if sys.path[0] == '':\n",
      "/Users/liwenxing/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_prep_data_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lined-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prep_data_2():\n",
    "    \"\"\"\n",
    "    Test if the prep_data function is responsive to a wrong datatype of the input.\n",
    "    \"\"\"\n",
    "    image_dir = 'data/images_10x'\n",
    "    input_1 = 10\n",
    "    test1 = False\n",
    "    try:\n",
    "        prep_data(input_1, image_dir)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test1 = True\n",
    "    assert test1 == True, \"Test failed! The prep_data function is not responsive to the wrong input datatype 'int'.\"\n",
    "    \n",
    "    input_2 = 1.2\n",
    "    test2 = False\n",
    "    try:\n",
    "        prep_data(input_2, image_dir)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test2 = True\n",
    "    assert test2 == True, \"Test failed! The prep_data function is not responsive to the wrong input datatype 'float'.\"\n",
    "    \n",
    "    input_3 = [1, 2, 3]\n",
    "    test3 = False\n",
    "    try:\n",
    "        prep_data(input_3, image_dir)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test3 = True\n",
    "    assert test3 == True, \"Test failed! The prep_data function is not responsive to the wrong input datatype 'list'.\"\n",
    "    \n",
    "    input_4 = (1, 2, 3)\n",
    "    test4 = False\n",
    "    try:\n",
    "        prep_data(input_4, image_dir)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test4 = True\n",
    "    assert test4 == True, \"Test failed! The prep_data function is not responsive to the wrong input datatype 'tuple'.\"\n",
    "    \n",
    "    input_5 = 'input'\n",
    "    test5 = False\n",
    "    try:\n",
    "        prep_data(input_5, image_dir)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test5 = True\n",
    "    assert test5 == True, \"Test failed! The prep_data function is not responsive to the wrong input datatype 'str'.\"\n",
    "    \n",
    "    input_6 = False\n",
    "    test6 = False\n",
    "    try:\n",
    "        prep_data(input_6, image_dir)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test6 = True\n",
    "    assert test6 == True, \"Test failed! The prep_data function is not responsive to the wrong input datatype 'bool'.\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "designing-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep_data_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "parallel-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_filenames(labels, image_root):\n",
    "    \"\"\"\n",
    "    Replaces sample column of labels with the actual filename so that the dataset class doesn't have to do that work.\n",
    "    \"\"\"\n",
    "    for i, item in labels['Sample'].iteritems():\n",
    "        if type(item) != list: \n",
    "            raise TypeError(\"The type of each item in 'Sample' column has to be 'list'.\") \n",
    "    image_filenames = os.listdir(image_root)\n",
    "    labels.insert(loc=1, column='File', value=None)\n",
    "    for index, row in labels.iterrows():\n",
    "        sample = row['Sample']\n",
    "        for fname in image_filenames:\n",
    "            str_id = '^' + ' '.join(row['Sample']) + ' .*'\n",
    "            result = re.search(str_id, fname)\n",
    "            if result:\n",
    "                image_file = result.group()\n",
    "                assert(os.path.exists('./data/images_10x/' + image_file))\n",
    "                break\n",
    "        else:\n",
    "            image_file = None\n",
    "        labels.loc[index, 'File'] = image_file\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "choice-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_filenames_1():\n",
    "    \"\"\"\n",
    "    Test if the add_filenames function can generate correct output.\n",
    "    \"\"\"\n",
    "    # Load the input data frame for the add_filenames function. \n",
    "    input_df = pd.read_csv('data/10x_labels_5.csv')\n",
    "    image_root = 'data/images_10x'\n",
    "    # Prepare the input data frame\n",
    "    sample_names = input_df[\"Sample\"].str.split(\" \", n=1, expand=False)\n",
    "    input_df['Sample'] = sample_names\n",
    "    # Call the add_filenames function and get the actual output data frame.\n",
    "    result = add_filenames(input_df, image_root)\n",
    "    # Load the output data frame. \n",
    "    output_df = pd.read_csv('data/10x_labels_5_output.csv')\n",
    "    # Modifiy the output data frame to make it expected output. \n",
    "    for i, rowi in output_df['Sample'].iteritems():\n",
    "        output_df['Sample'].loc[i] = rowi.split(',')\n",
    "    # Check if the expected output data frame is the same as the actual output data frame.\n",
    "    assert output_df.equals(result), \"The add_filenames function is broken!\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "violent-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_filenames_2():\n",
    "    \"\"\"\n",
    "    Test if the add_filenames function is responsive to a wrong datatype of the input.\n",
    "    \"\"\"\n",
    "    input_labels = pd.read_csv('data/10x_labels_5.csv')\n",
    "    image_root = 'data/images_10x'\n",
    "    test1 = False\n",
    "    try: \n",
    "        add_filenames(input_labels, image_root)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), 'Wrong type of error'\n",
    "        test1 = True\n",
    "    assert test1 == True, \"Test failed! The add_filenames function is not resposive to TypeErorr of each item in the 'Sample' column\"\n",
    "    \n",
    "    input_1 = 10\n",
    "    test2 = False\n",
    "    try:\n",
    "        add_filenames(input_1, image_root)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test2 = True\n",
    "    assert test2 == True, \"Test failed! The add_filenames function is not responsive to the wrong input datatype 'int'.\"\n",
    "    \n",
    "    input_2 = 1.2\n",
    "    test3 = False\n",
    "    try:\n",
    "        add_filenames(input_2, image_root)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test3 = True\n",
    "    assert test3 == True, \"Test failed! The add_filenames function is not responsive to the wrong input datatype 'float'.\"\n",
    "    \n",
    "    input_3 = [1, 2, 3]\n",
    "    test4 = False\n",
    "    try:\n",
    "        add_filenames(input_3, image_root)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test4 = True\n",
    "    assert test4 == True, \"Test failed! The add_filenames function is not responsive to the wrong input datatype 'list'.\"\n",
    "    \n",
    "    input_4 = (1, 2, 3)\n",
    "    test5 = False\n",
    "    try:\n",
    "        add_filenames(input_4, image_root)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test5 = True\n",
    "    assert test5 == True, \"Test failed! The add_filenames function is not responsive to the wrong input datatype 'tuple'.\"\n",
    "    \n",
    "    input_5 = 'input'\n",
    "    test6 = False\n",
    "    try:\n",
    "        add_filenames(input_5, image_root)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test6 = True\n",
    "    assert test6 == True, \"Test failed! The add_filenames function is not responsive to the wrong input datatype 'str'.\"\n",
    "    \n",
    "    input_6 = False\n",
    "    test7 = False\n",
    "    try:\n",
    "        add_filenames(input_6, image_root)\n",
    "    except Exception as e:\n",
    "        assert isinstance(e, TypeError), \"Wrong type of error.\"\n",
    "        test7 = True\n",
    "    assert test7 == True, \"Test failed! The add_filenames function is not responsive to the wrong input datatype 'bool'.\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sized-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_add_filenames_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indoor-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_add_filenames_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-bandwidth",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "magnetic-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tenX_dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Class inherited from torch Dataset. Required methods are, init,\n",
    "    len, and getitem.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_frame, image_dir, transform):\n",
    "        \"\"\"\n",
    "        initializes an instance of the class. Here we store 4 variables\n",
    "        in the class. Calling init just looks like dataset = tenX_dataset(lables, 'image_folder', transform).\n",
    "        \n",
    "        labels: altered version of csv file\n",
    "        image_dir: The file path to the folder the images are in\n",
    "        image_filenames: A list of all the image file names in the image folder\n",
    "        transform: A pytorch object. Works like a function. You call transform(x) and it performs\n",
    "                    a series of operations on x\n",
    "        \"\"\"\n",
    "        self.labels = labels_frame\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = os.listdir(self.image_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset\"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing image and image data. Right now\n",
    "        it looks like: \n",
    "        sample = {'image': image, 'plastic': [0], 'shape':[0,0,0,0,0], 'color':[0,0,0,0,0]}\n",
    "        \"\"\"\n",
    "        image_filename = self.labels['File'][idx]\n",
    "        image = None\n",
    "             \n",
    "        if image_filename is not None:\n",
    "            image_filepath = os.path.join(self.image_dir, image_filename)\n",
    "            image = skimage.io.imread(image_filepath)\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image,\n",
    "                  'shape': self.labels['Shape'][idx],\n",
    "                  'color': self.labels['Color'][idx],\n",
    "                  'plastic': self.labels['isPlastic'][idx]}\n",
    "  \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "every-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tenX_dataset():\n",
    "    \"\"\"\n",
    "    Test if the class tenX_dataset can generate correct output.\n",
    "     \n",
    "    \"\"\"\n",
    "    # Load the inputs for tenX_dataset.\n",
    "    image_dir = 'data/images_10x'\n",
    "    labels = prep_data(pd.read_csv('data/10x_labels_5.csv'), image_dir)\n",
    "    # To make the test more simple, define the \"transform\" as a function that returns \n",
    "    # the input directly without doing anything. \n",
    "    def transforms(image):\n",
    "        return image    \n",
    "    # Create an object tenX.\n",
    "    tenX = tenX_dataset(labels, image_dir, transforms)\n",
    "    # check if the class tenX_dataset can generate the correct output by comparing the actual output with the expected output.\n",
    "    assert len(tenX) == 10, \"The len method in class tenX_dataset is broken!\"\n",
    "    assert tenX[1]['image'].size == 1082880, \"The getitem method in class tenX_dataset is broken!\"\n",
    "    assert np.array_equal(tenX[1]['shape'], np.array([0, 0, 0, 1])), \"The getitem method in class tenX_dataset is broken!\"\n",
    "    assert np.array_equal(tenX[1]['color'], np.array([0, 0, 1, 0])), \"The getitem method in class tenX_dataset is broken!\"\n",
    "    assert tenX[1]['plastic'] == 0, \"The getitem method in class tenX_dataset is broken!\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "liberal-mileage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liwenxing/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/liwenxing/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "test_tenX_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-charlotte",
   "metadata": {},
   "source": [
    "### Plotting first 20 images of dataset. Obviously getting quite a few duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-endorsement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels_filepath = 'data/10x_labels_5.csv'\n",
    "image_dir = 'data/images_10x'\n",
    "pd_df = pd.read_csv(labels_filepath)\n",
    "labels = prep_data(pd_df, image_dir)\n",
    "#Probably wont center crop since all objects are always near middle of this. This will speed up the network\n",
    "#May want to consider translating images since elsewise our nn is biased to the center\n",
    "#Also option to change contrast with ImageEnhance https://pythonexamples.org/python-pillow-adjust-image-contrast/\n",
    "#two ways to normalize. Batch - noramlize wrt dataset mean/std, Indvid - noramlize each image with own mean and std\n",
    "#\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToPILImage(),\n",
    "                            torchvision.transforms.CenterCrop((300, 350)),\n",
    "                            torchvision.transforms.RandomRotation((-180,180)),\n",
    "                            torchvision.transforms.ToTensor()\n",
    "                                      ])\n",
    "tenX = tenX_dataset(labels, image_dir, transforms)\n",
    "\n",
    "for i in range(len(tenX)):\n",
    "    sample = tenX[i]['image']\n",
    "    plt.figure(i)\n",
    "    if sample is not None:\n",
    "        print(np.shape(sample))\n",
    "        plt.imshow(sample.T)\n",
    "    if i>100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-european",
   "metadata": {},
   "source": [
    "# Things to improve/fix\n",
    "* Make sure the nonetypes are because the file actually isn't in my folder of images\n",
    "* Code for normalizing image data\n",
    "* Image augmentation. Probably want to cut off some of the edges to get rid of number stuff and decrease extraneous information. The think we actually care about is only occupying like 5-10% of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-organizer",
   "metadata": {},
   "source": [
    "# Start of me trying to plug into cnn\n",
    "\n",
    "Most of the code came from this tutorial: https://github.com/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb\n",
    "\n",
    "I was just trying to get this to work so I won't understand it as much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'data/images_10x'\n",
    "labels_frame = labels\n",
    "\n",
    "#This transform just resizes the images to 3,480,752. So 3 for red green blue then height of 480\n",
    "#and width of 752. \n",
    "transform = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToPILImage(),\n",
    "                            torchvision.transforms.Resize((480, 752)),\n",
    "                            torchvision.transforms.ToTensor()\n",
    "                                      ])\n",
    "\n",
    "\n",
    "train_data = tenX_dataset(labels_frame, image_dir, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-killer",
   "metadata": {},
   "source": [
    "#### Splitting into train/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, \n",
    "                                           [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-rebecca",
   "metadata": {},
   "source": [
    "#### Declaring iterator. The thing that will loop through our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "train_iterator = torch.utils.data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = torch.utils.data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-determination",
   "metadata": {},
   "source": [
    "#### The CNN archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes CNN. Here we just define layer shapes that we call in the forward func\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        #Convulution layer 1. \n",
    "        #3 input channels (for three images Red, Green, Blue)\n",
    "        #6 output channels (I THINK this means we are applying two different filters to each image\n",
    "        #3 images, two filters each, we end up with 6 'images')\n",
    "        #kernel size is I THINK telling the filters took filter each set of 5 pixels into one.\n",
    "        #So are images will shrink a little as the edges get cut off\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, \n",
    "                               out_channels = 6, \n",
    "                               kernel_size = 5)\n",
    "        \n",
    "        #Convultion layer 2. See above\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, \n",
    "                               out_channels = 12, \n",
    "                               kernel_size = 5)\n",
    "        \n",
    "        #Linear layers. These probably arent complicated but I don't follow haha\n",
    "        #I think it turning the 259740 pixel values into 6 values. Then the second layers\n",
    "        #Turns the 6 into a different 6? and then 6 into 2. I'm not sure why 2 and not 1.\n",
    "        #Seeing as the output should be a number between 0-1. Closer to 0 = not plastic,\n",
    "        #closer to 1 = plastic. But I got errors about not having enough classes when\n",
    "        #I only had 1 output neuron.\n",
    "        #TBH these linear layers I just changed based on the error messages I got.\n",
    "        self.fc_1 = nn.Linear(259740, 6)\n",
    "        self.fc_2 = nn.Linear(6, 6)\n",
    "        self.fc_3 = nn.Linear(6, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Function that performs all the neural network forward calculation i.e.\n",
    "        takes image data from the input of the neural network to the output\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "    \n",
    "        #Gonna have to look at tutorial link.\n",
    "        x = nn.functional.max_pool2d(x, kernel_size = 2)\n",
    "        \n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "                \n",
    "        x = nn.functional.max_pool2d(x, kernel_size = 2)\n",
    "        \n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "                \n",
    "        h = x\n",
    "        \n",
    "        x = self.fc_1(x)\n",
    "                \n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "                \n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.fc_3(x)\n",
    "        \n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instancing model, loss criteria, device to perform calculations on, and optimizer.\n",
    "OUTPUT_DIM = 1\n",
    "model = LeNet(OUTPUT_DIM)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Telling the model and loss function to do math on whatever device is\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    Function calculate accuracy. See tutorial, may not\n",
    "    even be accurate for our model but it at least runs\n",
    "    \"\"\"\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop. Takes data through NN calculates loss and adjusts NN. Repeat\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    #Need to add logic to skip iteration if image is None\n",
    "    for sample in iterator:  \n",
    "        print('training')\n",
    "        if sample['image'] is None:\n",
    "            print('got a None')\n",
    "            continue\n",
    "        image = sample['image'].to(device)\n",
    "        isPlastic = sample['plastic'].to(device)\n",
    "                \n",
    "        optimizer.zero_grad()      \n",
    "        y_pred, what = model(image)\n",
    "\n",
    "        loss = criterion(y_pred, isPlastic)\n",
    "        acc = calculate_accuracy(y_pred, isPlastic)\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the model is actually trained\n",
    "EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-cliff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
