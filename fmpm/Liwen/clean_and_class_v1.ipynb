{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elegant-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional \n",
    "import torch.optim \n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision.transforms\n",
    "import torchvision.datasets\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-adventure",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sweet-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"sets seeds for several used packages\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "renewable-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_column(column):\n",
    "    \"\"\"\n",
    "    takes single columned Pandas DataFrame of categorical data and encodes it\n",
    "    into array of class binarys\n",
    "    \"\"\"\n",
    "    encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "    shape_arr = encoder.fit_transform(column).toarray().astype(int)\n",
    "        \n",
    "    return list(shape_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standard-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(labels, image_root):\n",
    "    \"\"\"\n",
    "    Takes in raw labels dataframe and converts it into the format\n",
    "    expected for tenX_dataset class\n",
    "    \"\"\"\n",
    "\n",
    "    #Splitting description column into color and shape columns\n",
    "    new = labels[\"Description\"].str.split(\" \", n=1, expand=True)\n",
    "    labels.drop(columns=['Description'], inplace=True)\n",
    "    labels['Color'] = new[0].values\n",
    "    labels['Shape'] = new[1].values\n",
    "    \n",
    "    #Decomposing sample keywords into seperate strings\n",
    "    sample_names = labels[\"Sample\"].str.split(\" \", n=1, expand=False)\n",
    "    labels['Sample'] = sample_names\n",
    "    \n",
    "    #Converting identification into boolean for is/is not plastic\n",
    "    PLASTICS = ['polystyrene', 'polyethylene','polypropylene','Nylon','ink + plastic','PET','carbon fiber']\n",
    "    identification = labels['Identification']\n",
    "    \n",
    "    for i in range(0,len(identification)):\n",
    "        if identification[i] in PLASTICS:\n",
    "            identification[i] = True\n",
    "        else:\n",
    "            identification[i] = False\n",
    "\n",
    "    labels['Identification']=identification\n",
    "    labels.rename(columns={'Identification': 'isPlastic'}, inplace=True)\n",
    "    labels['isPlastic'] = labels[\"isPlastic\"].astype(int)\n",
    "    \n",
    "    \n",
    "    #Encoding shape and color data\n",
    "    labels['Shape'] = encode_column(labels[['Shape']])\n",
    "    labels['Color'] = encode_column(labels[['Color']])\n",
    "    \n",
    "    labels = add_filenames(labels, image_root)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "insured-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_filenames(labels, image_root):\n",
    "    \"\"\"\n",
    "    Replaces sample column of labels with the actual filename so that the dataset class doesn't have to do that work.\n",
    "    \"\"\"\n",
    "    image_filenames = os.listdir(image_root)\n",
    "    labels.insert(loc=1, column='File', value=None)\n",
    "    for index, row in labels.iterrows():\n",
    "        sample = row['Sample']\n",
    "        for fname in image_filenames:\n",
    "            str_id = '^' + ' '.join(row['Sample']) + ' .*'\n",
    "            result = re.search(str_id, fname)\n",
    "            if result:\n",
    "                image_file = result.group()\n",
    "                assert(os.path.exists('./data/images_10x/' + image_file))\n",
    "                break\n",
    "        else:\n",
    "            image_file = None\n",
    "        labels.loc[index, 'File'] = image_file\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-cooking",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developmental-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tenX_dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Class inherited from torch Dataset. Required methods are, init,\n",
    "    len, and getitem.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_frame, image_dir, transform):\n",
    "        \"\"\"\n",
    "        initializes an instance of the class. Here we store 4 variables\n",
    "        in the class. Calling init just looks like dataset = tenX_dataset(lables, 'image_folder', transform).\n",
    "        \n",
    "        labels: altered version of csv file\n",
    "        image_dir: The file path to the folder the images are in\n",
    "        image_filenames: A list of all the image file names in the image folder\n",
    "        transform: A pytorch object. Works like a function. You call transform(x) and it performs\n",
    "                    a series of operations on x\n",
    "        \"\"\"\n",
    "        self.labels = labels_frame\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = os.listdir(self.image_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset\"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing image and image data. Right now\n",
    "        it looks like: \n",
    "        sample = {'image': image, 'plastic': [0], 'shape':[0,0,0,0,0], 'color':[0,0,0,0,0]}\n",
    "        \"\"\"\n",
    "        image_filename = self.labels['File'][idx]\n",
    "        image = None\n",
    "             \n",
    "        if image_filename is not None:\n",
    "            image_filepath = os.path.join(self.image_dir, image_filename)\n",
    "            image = skimage.io.imread(image_filepath)\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image,\n",
    "                  'shape': self.labels['Shape'][idx],\n",
    "                  'color': self.labels['Color'][idx],\n",
    "                  'plastic': self.labels['isPlastic'][idx]}\n",
    "  \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-border",
   "metadata": {},
   "source": [
    "### Plotting first 20 images of dataset. Obviously getting quite a few duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spiritual-synthetic",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/10x_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c90463b4c818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabels_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/10x_labels.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/images_10x'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtenX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtenX_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fmpm/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/10x_labels.csv'"
     ]
    }
   ],
   "source": [
    "labels_filepath = 'data/10x_labels.csv'\n",
    "image_dir = 'data/images_10x'\n",
    "labels = prep_data(pd.read_csv(labels_filepath), image_dir)\n",
    "tenX = tenX_dataset(labels, image_dir, None)\n",
    "\n",
    "\n",
    "for i in range(len(tenX)):\n",
    "    sample = tenX[i]\n",
    "    plt.figure(i)\n",
    "    if sample['image'] is not None:\n",
    "        plt.imshow(sample['image'])\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-drunk",
   "metadata": {},
   "source": [
    "# Things to improve/fix\n",
    "* Make sure the nonetypes are because the file actually isn't in my folder of images\n",
    "* Code for normalizing image data\n",
    "* Image augmentation. Probably want to cut off some of the edges to get rid of number stuff and decrease extraneous information. The think we actually care about is only occupying like 5-10% of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-engine",
   "metadata": {},
   "source": [
    "# Start of me trying to plug into cnn\n",
    "\n",
    "Most of the code came from this tutorial: https://github.com/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb\n",
    "\n",
    "I was just trying to get this to work so I won't understand it as much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "secret-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'data/images_10x'\n",
    "labels_frame = labels\n",
    "\n",
    "#This transform just resizes the images to 3,480,752. So 3 for red green blue then height of 480\n",
    "#and width of 752. \n",
    "transform = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToPILImage(),\n",
    "                            torchvision.transforms.Resize((480, 752)),\n",
    "                            torchvision.transforms.ToTensor()\n",
    "                                      ])\n",
    "\n",
    "\n",
    "train_data = tenX_dataset(labels_frame, image_dir, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-dimension",
   "metadata": {},
   "source": [
    "#### Splitting into train/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "periodic-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, \n",
    "                                           [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "motivated-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 56\n",
      "Number of validation examples: 7\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-debut",
   "metadata": {},
   "source": [
    "#### Declaring iterator. The thing that will loop through our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arbitrary-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "train_iterator = torch.utils.data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = torch.utils.data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-overall",
   "metadata": {},
   "source": [
    "#### The CNN archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "surgical-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes CNN. Here we just define layer shapes that we call in the forward func\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        #Convulution layer 1. \n",
    "        #3 input channels (for three images Red, Green, Blue)\n",
    "        #6 output channels (I THINK this means we are applying two different filters to each image\n",
    "        #3 images, two filters each, we end up with 6 'images')\n",
    "        #kernel size is I THINK telling the filters took filter each set of 5 pixels into one.\n",
    "        #So are images will shrink a little as the edges get cut off\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, \n",
    "                               out_channels = 6, \n",
    "                               kernel_size = 5)\n",
    "        \n",
    "        #Convultion layer 2. See above\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, \n",
    "                               out_channels = 12, \n",
    "                               kernel_size = 5)\n",
    "        \n",
    "        #Linear layers. These probably arent complicated but I don't follow haha\n",
    "        #I think it turning the 259740 pixel values into 6 values. Then the second layers\n",
    "        #Turns the 6 into a different 6? and then 6 into 2. I'm not sure why 2 and not 1.\n",
    "        #Seeing as the output should be a number between 0-1. Closer to 0 = not plastic,\n",
    "        #closer to 1 = plastic. But I got errors about not having enough classes when\n",
    "        #I only had 1 output neuron.\n",
    "        #TBH these linear layers I just changed based on the error messages I got.\n",
    "        self.fc_1 = nn.Linear(259740, 6)\n",
    "        self.fc_2 = nn.Linear(6, 6)\n",
    "        self.fc_3 = nn.Linear(6, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Function that performs all the neural network forward calculation i.e.\n",
    "        takes image data from the input of the neural network to the output\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "    \n",
    "        #Gonna have to look at tutorial link.\n",
    "        x = nn.functional.max_pool2d(x, kernel_size = 2)\n",
    "        \n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "                \n",
    "        x = nn.functional.max_pool2d(x, kernel_size = 2)\n",
    "        \n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "                \n",
    "        h = x\n",
    "        \n",
    "        x = self.fc_1(x)\n",
    "                \n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "                \n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.fc_3(x)\n",
    "        \n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "biblical-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instancing model, loss criteria, device to perform calculations on, and optimizer.\n",
    "OUTPUT_DIM = 1\n",
    "model = LeNet(OUTPUT_DIM)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beautiful-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Telling the model and loss function to do math on whatever device is\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "automatic-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    Function calculate accuracy. See tutorial, may not\n",
    "    even be accurate for our model but it at least runs\n",
    "    \"\"\"\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "continent-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop. Takes data through NN calculates loss and adjusts NN. Repeat\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    #Need to add logic to skip iteration if image is None\n",
    "    for sample in iterator:  \n",
    "        \n",
    "        image = sample['image'].to(device)\n",
    "        isPlastic = sample['plastic'].to(device)\n",
    "                \n",
    "        optimizer.zero_grad()      \n",
    "        y_pred, what = model(image)\n",
    "\n",
    "        loss = criterion(y_pred, isPlastic)\n",
    "        acc = calculate_accuracy(y_pred, isPlastic)\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "desirable-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n",
      "tensor([[[[0.9333, 0.9333, 0.9412,  ..., 0.9451, 0.9490, 0.9529],\n",
      "          [0.9373, 0.9373, 0.9412,  ..., 0.9333, 0.9333, 0.9490],\n",
      "          [0.9373, 0.9373, 0.9412,  ..., 0.9412, 0.9373, 0.9451],\n",
      "          ...,\n",
      "          [0.9373, 0.9373, 0.9412,  ..., 0.8980, 0.9059, 0.9176],\n",
      "          [0.9333, 0.9333, 0.9373,  ..., 0.9059, 0.9059, 0.9216],\n",
      "          [0.9333, 0.9333, 0.9373,  ..., 0.9059, 0.9137, 0.9216]],\n",
      "\n",
      "         [[0.3961, 0.4431, 0.4824,  ..., 0.4275, 0.4314, 0.4471],\n",
      "          [0.4000, 0.4196, 0.4392,  ..., 0.4118, 0.4196, 0.4431],\n",
      "          [0.4000, 0.3961, 0.3882,  ..., 0.4196, 0.4275, 0.4353],\n",
      "          ...,\n",
      "          [0.3922, 0.3922, 0.3922,  ..., 0.3529, 0.3725, 0.3804],\n",
      "          [0.3922, 0.3961, 0.3843,  ..., 0.3725, 0.3529, 0.3765],\n",
      "          [0.3961, 0.3882, 0.3961,  ..., 0.3725, 0.3647, 0.3647]],\n",
      "\n",
      "         [[0.1765, 0.2000, 0.2235,  ..., 0.2706, 0.2627, 0.2627],\n",
      "          [0.1765, 0.2000, 0.2235,  ..., 0.2627, 0.2627, 0.2627],\n",
      "          [0.2039, 0.2039, 0.2078,  ..., 0.2471, 0.2471, 0.2549],\n",
      "          ...,\n",
      "          [0.1843, 0.1882, 0.1961,  ..., 0.1843, 0.2078, 0.2078],\n",
      "          [0.1647, 0.1804, 0.1922,  ..., 0.1529, 0.1686, 0.1725],\n",
      "          [0.1373, 0.1686, 0.1961,  ..., 0.1529, 0.1333, 0.1333]]],\n",
      "\n",
      "\n",
      "        [[[0.9804, 0.9804, 0.9882,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9765, 0.9765, 0.9843,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9725, 0.9725, 0.9804,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9804, 0.9725],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9843, 0.9804],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9882, 0.9804]],\n",
      "\n",
      "         [[0.4235, 0.4627, 0.5020,  ..., 0.5059, 0.4980, 0.5098],\n",
      "          [0.4235, 0.4431, 0.4824,  ..., 0.4902, 0.4980, 0.5098],\n",
      "          [0.4235, 0.4353, 0.4471,  ..., 0.4980, 0.5059, 0.5176],\n",
      "          ...,\n",
      "          [0.4667, 0.4667, 0.4706,  ..., 0.4275, 0.4275, 0.4431],\n",
      "          [0.4549, 0.4588, 0.4667,  ..., 0.4392, 0.4353, 0.4471],\n",
      "          [0.4471, 0.4510, 0.4549,  ..., 0.4431, 0.4471, 0.4431]],\n",
      "\n",
      "         [[0.2314, 0.2431, 0.2627,  ..., 0.3333, 0.3294, 0.3294],\n",
      "          [0.2314, 0.2431, 0.2627,  ..., 0.3216, 0.3294, 0.3294],\n",
      "          [0.2627, 0.2627, 0.2627,  ..., 0.3216, 0.3216, 0.3255],\n",
      "          ...,\n",
      "          [0.2588, 0.2588, 0.2588,  ..., 0.2471, 0.2353, 0.2353],\n",
      "          [0.2471, 0.2471, 0.2510,  ..., 0.2353, 0.2353, 0.2392],\n",
      "          [0.2353, 0.2392, 0.2471,  ..., 0.2510, 0.2471, 0.2471]]],\n",
      "\n",
      "\n",
      "        [[[0.9765, 0.9765, 0.9882,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9804, 0.9804, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9843, 0.9843, 0.9922,  ..., 0.9961, 0.9961, 1.0000],\n",
      "          ...,\n",
      "          [0.9765, 0.9765, 0.9725,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9725, 0.9725, 0.9647,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9725, 0.9725, 0.9647,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[0.4667, 0.4980, 0.5216,  ..., 0.4941, 0.4863, 0.4941],\n",
      "          [0.4627, 0.4824, 0.4980,  ..., 0.4745, 0.4824, 0.4863],\n",
      "          [0.4510, 0.4510, 0.4549,  ..., 0.4588, 0.4667, 0.4784],\n",
      "          ...,\n",
      "          [0.4118, 0.4157, 0.4235,  ..., 0.4510, 0.4667, 0.4824],\n",
      "          [0.4157, 0.4157, 0.4157,  ..., 0.4510, 0.4588, 0.4745],\n",
      "          [0.4196, 0.4196, 0.4196,  ..., 0.4627, 0.4667, 0.4667]],\n",
      "\n",
      "         [[0.2235, 0.2588, 0.2902,  ..., 0.2824, 0.2902, 0.2902],\n",
      "          [0.2235, 0.2588, 0.2902,  ..., 0.2784, 0.2902, 0.2902],\n",
      "          [0.2314, 0.2471, 0.2627,  ..., 0.2627, 0.2706, 0.2784],\n",
      "          ...,\n",
      "          [0.2235, 0.2275, 0.2353,  ..., 0.2627, 0.2706, 0.2706],\n",
      "          [0.2078, 0.2039, 0.2000,  ..., 0.2627, 0.2471, 0.2627],\n",
      "          [0.1961, 0.1843, 0.1686,  ..., 0.2667, 0.2588, 0.2588]]],\n",
      "\n",
      "\n",
      "        [[[0.9804, 0.9804, 0.9725,  ..., 0.9961, 1.0000, 1.0000],\n",
      "          [0.9569, 0.9569, 0.9569,  ..., 0.9882, 0.9961, 0.9961],\n",
      "          [0.9333, 0.9333, 0.9373,  ..., 0.9922, 0.9961, 0.9961],\n",
      "          ...,\n",
      "          [0.9137, 0.9137, 0.9176,  ..., 0.9922, 0.9804, 0.9725],\n",
      "          [0.9059, 0.9059, 0.9137,  ..., 1.0000, 0.9922, 0.9647],\n",
      "          [0.9059, 0.9059, 0.9137,  ..., 1.0000, 0.9922, 0.9647]],\n",
      "\n",
      "         [[0.4275, 0.4627, 0.4902,  ..., 0.4627, 0.4549, 0.4627],\n",
      "          [0.4235, 0.4431, 0.4588,  ..., 0.4431, 0.4510, 0.4627],\n",
      "          [0.4078, 0.4078, 0.4039,  ..., 0.4510, 0.4588, 0.4706],\n",
      "          ...,\n",
      "          [0.3804, 0.3686, 0.3804,  ..., 0.4000, 0.4118, 0.4353],\n",
      "          [0.3882, 0.3804, 0.3725,  ..., 0.4118, 0.4196, 0.4353],\n",
      "          [0.3961, 0.3804, 0.3765,  ..., 0.4196, 0.4235, 0.4275]],\n",
      "\n",
      "         [[0.1804, 0.2000, 0.2235,  ..., 0.2588, 0.2314, 0.2314],\n",
      "          [0.1804, 0.2000, 0.2235,  ..., 0.2588, 0.2314, 0.2314],\n",
      "          [0.1961, 0.2078, 0.2235,  ..., 0.2471, 0.2431, 0.2510],\n",
      "          ...,\n",
      "          [0.1804, 0.1804, 0.1804,  ..., 0.1961, 0.1961, 0.1961],\n",
      "          [0.1686, 0.1686, 0.1686,  ..., 0.1843, 0.1843, 0.1922],\n",
      "          [0.1490, 0.1529, 0.1529,  ..., 0.1922, 0.1961, 0.1961]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[0.4980, 0.5333, 0.5608,  ..., 0.5059, 0.5020, 0.5098],\n",
      "          [0.5020, 0.5176, 0.5333,  ..., 0.4902, 0.5059, 0.5098],\n",
      "          [0.4863, 0.4902, 0.4902,  ..., 0.4980, 0.5059, 0.5176],\n",
      "          ...,\n",
      "          [0.4549, 0.4549, 0.4627,  ..., 0.4824, 0.4902, 0.5020],\n",
      "          [0.4549, 0.4549, 0.4549,  ..., 0.4824, 0.4824, 0.4941],\n",
      "          [0.4549, 0.4549, 0.4510,  ..., 0.4863, 0.4824, 0.4824]],\n",
      "\n",
      "         [[0.2706, 0.2863, 0.2980,  ..., 0.3216, 0.3216, 0.3216],\n",
      "          [0.2706, 0.2863, 0.2980,  ..., 0.3098, 0.3216, 0.3216],\n",
      "          [0.2784, 0.2784, 0.2745,  ..., 0.3098, 0.3216, 0.3216],\n",
      "          ...,\n",
      "          [0.2549, 0.2510, 0.2471,  ..., 0.2902, 0.2902, 0.2902],\n",
      "          [0.2510, 0.2471, 0.2431,  ..., 0.2706, 0.2706, 0.2745],\n",
      "          [0.2471, 0.2431, 0.2431,  ..., 0.2706, 0.2627, 0.2627]]]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0ba7e9a58b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-4074d6294087>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6fe4b5b658c8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         sample = {'image': image,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be Tensor or ndarray. Got {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "#Here the model is actually trained\n",
    "EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-criterion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
